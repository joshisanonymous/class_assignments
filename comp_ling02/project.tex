%%%%%%%%%%%%%%%%%%%%%%%%
% Compile with XeLaTeX %
%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
  % Packages and settings
  \usepackage{fontspec}
    \setmainfont{Charis SIL}
  \usepackage[style=apa, backend=biber]{biblatex}
    \addbibresource{References.bib}
  \usepackage{hyperref}

  % Document information
  \title{Review of language variety identification}
  \author{Joshua McNeill}
  \date{\today}

  % New commands
  \newcommand{\orth}[1]{$\langle$#1$\rangle$}
  \newcommand{\lexi}[1]{\textit{#1}}
  \newcommand{\gloss}[1]{`#1'}

\begin{document}
  \maketitle
  \section{Introduction}
    A task that has received significant attention in natural language processing is language identification.
    This task involves classifying documents according to their language.
    As such, language identification is also a document classification task, though there are some features that are particular to this task.
    \textcite{jauhiainen_automatic_2019} explain, whereas in other classification tasks one can tokenize into words under a single, reliable scheme, in language identification one is working with various languages which may not represent word boundaries in the same way in writing.
    Language identification is also more likely to require domain independent solutions than, for instance, classifying news articles where the sort of documents being classified is relatively consistent even if the topics are not.
    By contrast, language identification may involve documents as diverse as tweets on Twitter to blog posts to books.
    Relatedly, a single language may even be written using multiple orthographies when used in various domains.
    Where one may write \orth{u} in a tweet, the standard \orth{you} spelling is expected in formal documents, or where Roman-based pinyin may be acceptable in some contexts for Chinese writing, Chinese characters are likely required in others.
    Finally, whereas a single news article, for instance, may be given two separate classes at once, the classes for language identification are necessarily mutually exclusive \parencite[p.~678]{jauhiainen_automatic_2019}.
    Certainly, there are cases of code-switching in writing, where one can argue that two separate languages are being used simultaneously, but the goal in these cases would be either to split the document up into sub-documents by language or to simply treat the resulting mixed code as its own distinct language.

    Language identification has been worked on as far back as the 1960s with \textcite{gold_language_1967} and Mustonen \parencite[1965, as cited in][]{jauhiainen_automatic_2019}, the latter of which involved using a dictionary language model to classify individual words as English, Swedish or Finnish, which yielded an accuracy of 76\% (p.~679).
    Later, Rau (1974) introduced the idea of using the relative frequencies of unigrams and bigrams in a language to construct a language model that could be used to identify a document as being in either English or Spanish, achieving an accuracy of 89\% at the time \parencite[as cited in][p.~680]{jauhiainen_automatic_2019}.
    By the 1990s, \citeauthor{cavnar_n-gram-based_1994} (\citeyear{cavnar_n-gram-based_1994}) approach to using $n$-grams for language identification became popular as their method yielded 99.8\% accuracy when distinguishing between eight languages.
    In sum, the history of work on this task has been long and led to results that suggest that the problem has been solved, though there is more work to be done still.

    Part of what makes language identification important today is the breadth of languages used on the internet.
    For instance, although 51\% of the tweets on the social media platform Twitter are in English, the other 49\% are in a large variety of other languages.
    Indeed, Japanese makes up 19.1\% of the remainder, the second highest amount, but there is a drastic drop in the percentage that any single language makes up as one continues down the list \parencite[p.~519]{hong_language_2011}.

    Being able to automatically classify texts online into one language or another can advance understanding of how the internet is used and even shed light on social issues, but there is also practical reasons for taking on this challenge.
    In general, language identification is a first step to be taken before many other types of natural language processing can happen.
    For example, if one wishes to develop a part-of-speech tagger for Spanish, it is important to identify documents that are indeed in Spanish.
    Online texts are also a rich source for constructing corpora, particularly for low-resource languages.
    Where they may not be a literary tradition for a language, there may still be informal writing in the form of tweets, blogs, and web pages.
    Forming corpora from these resources is far easier when their presence can be detected automatically with language identification techniques.

    % Brings up the problem is classifying very short texts, as well
      % In the case of Duvenhage (2019), a short text that we'd want to classify may be 15-20 characters long (1)

    An issue that is not as often addressed in work on language identification, however, is how languages are defined to begin with.
    In general when working on this task, researchers rely on well known labels for social constructs such as ``English'' or ``Spanish'', rarely taking a critical look at this aspect of their work.
    This lack of critique is understandable given goals that are often very practical as opposed to philosophical.
    However, from a sociolinguistic point of view, just what counts as a language is a much bigger question.
    One may work under the assumption that ``English'' is a language, but this raises the question of to what extent the English of a New Yorker is the same language as the English of a South African or a Scotsman or, to take it even further, an English-based creole like Gullah along the southeast coast of the United States or Jamaican Creole.
    There is mutual intelligibility in many cases, but that does not always coincide with language delineations.
    A common example is that Swedish and Norwegian are considered two separate languages while being generally mutually intelligible, but there are many ``dialects'' of Chinese that are not mutually intelligible at all.

    For the broad task of language identification, these issues are not addressed and instead form a set of assumptions underlying researchers' work, but they are in fact taken up to a degree in work on language variety identification.
    Language variety identification narrows the task by focusing on classifying different varieties of a single language.
    ``Variety'' in this case is often taken at a national level.
    One might work on distinguishing between British, Canadian, and Australian English \parencite{lui_classifying_2013} or Argentinian, Chilean, Colombian, Mexican, and Spain Spanish \parencite{maier_language_2014,rangel_low_2018}.
    Notably, the Discriminating Similar Languages (DSL) competition involved mostly pairs of national varieties along these lines, which included distinguishing between Malay and Indonesian, Brazilian and European Portuguese, national Spanish varieties, Farsi and Dari, and Croatian, Serbian and Slovene \parencite[p.~35]{malmasi_language_2015}.
    This work does get more fine-grained, however.
    For example, \textcite{ragab_mawdoo3_2019} worked on the problem of correctly identifying 26 different dialects of Arabic, each associated with a particular city, and \textcite{malmasi_german_2017} and \parencite{ciobanu_german_2018} worked on distinguishing between four varieties of Swiss German, also each associated with a particular city.

    \subsection{Related Issues}
      The language variety identification task heightens several already difficulties that exist with language identification in general.
      One of those is the problem of low-resource langauges.
      As mentioned above, good language identification systems can be used to collect documents that can then be formed into corpora for low-resource languages that might not have formal literary traditions, but in a catch-22, it is difficult to develop a classifier for such languages without collected documents to begin with.
      Naturally, the low-resource language problem is more common when one is working with a narrower set of varieties.
      Whereas \textcite{qi_study_2019} suggsted Indonesian, Kazakh, and Tibetan as examples of low-resource languages (p.~1897), and \textcite{duvenhage_short_2019} suggested some South African languages (p.~1), one can expect effectively all varieties more fine-grained than national varieties to be low-resource in language variety identification.
      The degree to which there is a literary tradition for Punjabi-influenced West London English \parencite{sharma_style_2011} or Californian nerd girl English \parencite{bucholtz_why_1999} is unarguably limited.

      Additionally, there is perhaps more of a need to deal with code-switching when working with language variety identification than with general language identification.
      Code-switching does of course occur with language varieties at any level of abstraction -- one can speak of English-Spanish code-switching as a common phenomenon, for example \parencite{toribio_spanish-english_2002} -- though there is an argument to be made that documents containing heavy code-switching may in fact be composed of a single language variety, particularly when the switching is intra-sentential, occurring within a single sentence, whereas in more general language identification, the task here would naturally be to isolate chunks of a document as one language or another.
      Indeed, varieties such as Chiac, originating from a heavy mixture of English and French, are generally considered to be language varieties in their own right \parencite{king_chiac_2008}.
      For example, \textcite{barman_code_2014} worked on language identification in Facebook posts by multilinguals where Bengali, Hindi, and English were present (p.~14).
      As they were starting from a broad conceptualization of what a language is, they described their task as separating out the languages of any given post even when the code-switching occurs from word to word (p.~13), a situation that is difficult to distinguish from borrowings of individual words into a language.\footnote{
        See \textcite{deuchar_english-origin_2016} for a relatively recent overview of the arguments in sociolinguistics.
      }

      In particular, when working with language identification, it is common to apply these systems to internet domains where code-switching is also common.
      English-Chinese bilinguals have been documented regularly code-switching as far back as when ICQ, a relatively archaic instant messenger by today's standards, was still widely used \parencite{ho_functional_2006}.
      The practice also occurs even in independent chat rooms deployed for experiments, as \textcite{cardenas-claros_code-switching_2009} found in their work with English-Spanish and English-In\-do\-ne\-sian bilinguals.
      This code-switching can take the form of inter-sentential switching as Hidayat (2012) found on Facebook \parencite[as cited in][p.~14]{barman_code_2014}, where adjacent sentences are in different languages, but intra-sentential switching can also be prevalent, as \textcite{san_chinese-english_2009} found in blogs written by Chinese-English bilinguals in Macao, China (p.~12).

      The presence of code-switching at times can exacerbate language variety identification even further.
      As already mentioned, having to work with multiple orthographies and writing norms is a particular issue for this task over other classification tasks.
      \textcite{barman_code_2014} found a special case of this where their participants would sometimes write Bengali and Hindi using the Roman alphabet, where they would normally be written in other scripts, to facilitate typing where English was also present (p.~13).
      Their solution to this was to exclude anything not written this way so as to avoid dealing with multiple scripts \parencite[p.~15]{barman_code_2014}.
      This solution is not ideal but perhaps helps when dealing with multiple scripts.
      More such solutions and approaches to language variety identification will be covered in the next section.

      % Orthographies
        % This is not uncommon for various scripts and even different encodings, as Jauhiainen et al. (2019) acknowledge (682-683)
      % Where multiple spellings are possible, these can be normalized
        % Barman et al. (2014) used the Lexical Normalization List (LexNormList) (Han et al. 2012) to do this (17)

  \section{Approaches}
    To some degree, language identification is a task that has been solved.
    After all, \textcite{cavnar_n-gram-based_1994} long ago achieved an accuracy of 99.8\% with their approach.
    However, it must be remembered that their test involved one domain, Usenet group messages from soc.culture (i.e., one topic from a sort of precursor to online forums), and perhaps more importantly, their system could identify eight fairly distinct languages \parencite[pp.~6-7]{cavnar_n-gram-based_1994}.
    That is not to say that this was not a great achievement, but language variety identification is an inherently more difficult task as the varieties being distinguished are inherently similar to each other.
    As such, this section will cover the corpora used to both train and test language variety identification systems, the types of language models employed, and the classifier algorithms used to make the final classification decisions.

    \subsection{Corpora}
      The corpora used to train language variety identification systems are key elements given that the goals for this task often include out-of-domain functionality.
      As \textcite{lui_classifying_2013} demonstrated, training their system on balanced national corpora can lead to very bad results when the system is applied to an unincluded domain such as Twitter (pp.~11-12), despite the large size of these corpora.
      As such, researchers employ various types of corpora.

      Constructing large-scale corpora has traditional been a labor intensive and expensive job.
      \textcite{lui_classifying_2013}, as mentioned, made use of balanced national corpora in their work on English varieties (p.~8): the British Natioal Corpus (BNC) for the United Kingdom \parencite{bnc_consortium_british_2007} and the Strathy Corpus for Canada \parencite{noauthor_strathy_nodate}.
      Both the BNC and the Strathy Corpus were constructed without the internet as a resource in mind, so while each includes enough domains to be considered balanced, they do not translate well for tasks that involve internet documents.
      Each includes written documents from fiction, magazines, news articles, and academic texts, as well as some spoken language transcriptions.
      Not all of these large-scale corpora are so balanced, though.
      Corpora like WordNet, a dictionary of English lexical items where those in sense relations are linked includes 166,000 such items \parencite{miller_wordnet_1995}.
      Dictionaries such as this can be used as language models just as well as balanced corpora of documents.

      While the aforementioned corpora were intially large, complex projects in and of themselves, they were all constructed and made available to later researchers.
      This greatly lowers the cost for training and testing new language models, but some flexibility is also lost.
      Others have instead built large, expensive corpora specifically for the task of language variety identification and often made them available to other researchers through competitions.
      For example, \textcite{bouamor_madar_2019} held a recent competition for identifying varieties of Arabic.
      They build the MADAR Travel Domain Corpus for said competition, which is a translation of the Basic Travel Expression Corpus (BTEC) which is a parallel corpus for common expressions in English, Japanese, and Chinese \parencite{takezawa_multilingual_2007}.
      The MADAR Travel Domain Corpus involved translating these expressions into 26 dialects of Arabic, each associated with a city except for Modern Standard Arabic.
      This limits the corpus to one domain but with a breadth of varieties that does not exist for those working with other languages.

      Similarly, \textcite{zampieri_findings_2017} constructed the Discriminating between Similar Languages (DSL) corpus that included 22,000 short news articles each for 14 different languages that form pairs of similar varieties.
      They then employed the corpus in a language variety identification competition, in fact not the first of which they conducted in only a few short years.
      This corpus was actually an extension of previous corpora used for the same task.
      While the DSL does not have the size of corpora like the BNC nor the breadth of related varieties like the MADAR Travel Domain Corpus, it is an important corpus for its specific focus on language variety identification.

      Others have constructed their own corpora for the language variety identification task but by using internet resources.
      Despite the noisiness of internet data, this approach has two benefits: Large corpora can be constructed with relative ease, and training on internet documents translates into more effective systems for identifying language varieties in internet domains than traditional corpora.
      In particular, Twitter has been repeatedly utilized this way.
      \textcite{bouamor_madar_2019}, for instance, not only used their MADAR Travel Domain Corpus for their Arabic competition but also a Twitter corpus for a second portion of the competition: the MADAR Twitter User Dialect Identification corpus.
      To construct this corpus, they identified relevant Twitter users by their use of hashtags naming countries where Arabic is spoken, such as \#Egypt.
      They do not note the fact that they searched for Arabic speakers using English hashtags, though it is possible that hashtags do not function with Arabic script.
      Otherwise, it would be reasonable to assume these are all bilinguals, as well.
      Users were then annotated manually for their city and the most recent 100 tweets from each of those residing in the 25 analyzed cities were saved to form the corpus.

      Likewise, \textcite{maier_language_2014} also used Twitter as a resource for constructing a corpus in their work on identifying varieties of Spanish.
      In their case, they used tweets that were geotagged to ensure that they originated in countries of interest.
      Two issues arose in their work.
      Geotagged tweets are both rare and not a guarantee that the speaker is from the place from which they are currently tweeting, which form some caveats for their corpus.
      Additionally, not all of the tweets collected were in Spanish.
      They decided to keep these non-Spanish tweets, explaining that any tweet ``in a certain country captures the variant of Spanish that is spoken'' (p.~27), suggesting that they conceptualized code-switching as leading to a distinct variety in and of itself.
      Their corpus ultimately resulted in 100,000 tweets from each of 5 countries: Argentina, Chile, Colombia, Mexico, and Spain.

      \textcite{franco-salvador_language_2015} and \textcite{rangel_low_2018} both worked on Spanish using the four of the five countries that \textcite{maier_language_2014} used, only switching out Colombia for Peru.
      Instead of Twitter, however, they constructed a corpus from blogs that they called HispaBlogs.
      While they identified blogs manually by looking for prominent bloggers in each country, the collection and processing of the texts could be done automatically, making this another relatively easier way to construct a corpus than that which was done for a corpus like the BNC.
      This corpus has the benefit of still being based on internet documents but including documents that are much longer than tweets.
      Indeed, very short documents are exceedingly difficult to classify.
      \textcite{maier_language_2014}, for example, showed a general trend towards higher $F_1$ scores as the length of the tweet being classified increases (p.~29).

      Finally, some have sought to cover as many domains as possible by combining several types of corpora, as as \textcite{lui_classifying_2013} in their work on English varieties.
      As already mentioned, they made use of the BNC and the Strathy Corpus, but they augmented these corpora by mining the web for documents as well as Twitter.
      For the web, they used BootCaT, a bootstrapping package, to generate 3-tuples from frequent words used in the BNC which they could then use as search queries.
      They specifically limited their queries to URLs that ended in .au, .ca, and .uk, representing Australian, Canadian, and British web sites, respectively.
      They also used the added .gov to each (i.e., .gov.au, .gov.ca, and .gov.uk) to obtain government documents specifically.
      These documents thus formed the web and government web sections of their corpus.
      Additionally, they used geotagged tweets to collect data from Twitter from each country.
      The end result was a quite large yet relatively easily created corpus that can presumably handle traditional documents as well as the range of documents types that might be found on the internet.

    \subsection{Language Models}
      With corpora available for training language variety identification systems, various language models can now be employed.
      In general, language models for this task are sets of types and, typically, the token frequencies for those types collected from some corpus or corpora.
      What counts as a type, however, can vary, and difference approaches can lead to different results for the classifier.

      A rather successful approach to the broader task of language identification involved building character $n$-gram language models \parencite{cavnar_n-gram-based_1994}, so a natural way to approach language variety identification is to do the same thing.
      This formed one of the approaches that \textcite{maier_language_2014} tested.
      In fact, they used TextCat for one of those tests, which is a popular implementation of \citeauthor{cavnar_n-gram-based_1994}'s (\citeyear{cavnar_n-gram-based_1994}) model.
      The idea behind this model is to take every combination of $n$ characters in the training set as types, though \textcite{maier_language_2014} do not state what their $n$ was.
      In their case, they used a simple frequency distribution as the language model, counting up how often each type showed up in the training set.
      The overal $F_1$ using this model was only 34.08, so they also tested another character $n$-gram model using absolute discounting in order to deal with types that had counts of zero.
      They compared models from unigram to 6-gram, finding the best results with the 6-gram model: an $F_1$ of 66.96.
      This was clearly better than using a frequency distribution alone, but left much room for improvement.


      % Character n-gram approach
        % Barman et al. (2014) achieved 94.62% accuracy with character n-grams alone, and this when very slightly up when included other context-free features as well (19)
          % These also went up roughly another 1% when adding context by using Conditional Random Fields (Barman et al. 2014:20)

      In an attempt improve this system further, \textcite{maier_language_2014} also tested a syllable $n$-gram language model.
      Syllabification, the process of splitting words up into syllables, is not an easy task itself, particularly when working with casual online writing where standard orthographic norms may not be followed such as using diacritics, an issue that \textcite{maier_language_2014} noted.
      Nonetheless, they utilized the TIP syllabifier for this task, which is based on the phonotactic rules set out by the Royal Spanish Academy of Language.
      Their rationale for this step was that ``the errors originated by unaccented words will follow a uniform pattern'' (p.~30).
      The results they achieved using this language model with absolute discounting again did not match the results with the character $n$-gram approach, reaching an $F_1$ of 57.88 overall with syllable 4-grams.

      Similar to character $n$-gram models are byte $n$-gram models as used by \textcite{lui_classifying_2013} and, once again, \textcite{maier_language_2014}.
      Characters are in fact read by computers as sequences of bytes.
      Depending on the character encoding used, a two characters that look identical to the human eye may in fact be represented by different byte sequences and, as a result, treated as two different things by a computer.
      In essence, then, byte $n$-gram models are simply more precise versions of character $n$-gram models.
      \textcite{lui_classifying_2013} achieved a reasonable $F_1$ of 0.772 with this model when applied to traditional documents, but this number dropped off drastically when applied to web documents (0.538) and especially Twitter (0.043).
      For \textcite{maier_language_2014}, this yielded very poor results, though this may be due to the use of compression to decide on a classification, which will be discussed in the next section.

      A natural progression from characters and syllables is to move to the next larger linguistic units: words.
      In this case, researchers are still using $n$-gram models, but the types are words.
      \textcite{lui_classifying_2013} tested two versions of word-based models: one that included all words from the training set and one that included only function words.
      The function word model in fact had some of the worst performance out of any models they attempted, only achieving an $F_1$ of 0.502 even with traditional documents.
      In contrast, the model that included all words resulted in the highest performance across the board: an $F_1$ of 0.911 for traditional documents, 0.656 for web documents, and 0.447 for Twitter.
      The difference seems to suggest that function words are not a common place of variation between national varieties of English.

      \textcite{franco-salvador_language_2015}, working national varieties of Spanish, also used a word-based model, though the $n$-grams they used were instead Skip-grams.
      A Skip-gram attempts to capture long distance dependencies in a sentence better than standard $n$-grams by making each type a sequence of words that skips $X$ words.
      In other words, a the Skip-grams generated by a sentence like ``the dog ate her food'' when taking two words and one skip would be ``the ate'', ``dog her'', and ``ate food''.
      This approach resulted in an accuracy of 0.722, better than the comparable domain of web documents used in \textcite{lui_classifying_2013} where $F_1$ was 0.656.
      However, while web documents may include blogs as used in \textcite{franco-salvador_language_2015}, blogs will not include web documents that may be more difficult to classify.

      Also working on Spanish, \textcite{rangel_low_2018} aimed to achieve high accuracy while also greatly reducing the feature space.
      Their process involved using word $n$-grams but filtering down the set of types to only 6 key words through a process they call low dimensionality representation (LDR).
      This involves a series of weight calculations where low weight words are removed between every calculation.
      Compared to a Skip-gram approach that they also attempted, this yielded a model with only 30 types as opposed to the Skip-gram model with 300 types.
      Accuracy was also highly comparable at 0.711, suggesting that at least for Spanish, important vocabulary items are indeed major distinguishing features for national varieties.

      % Dictionary approach
        % Barman et al. (2014) achieved 93.12% accuracy using training data and LexNormList as dictionaries (19)

      As opposed to relying on a single language model, it is also possible to employ multiple models at the same time as part of a classifier ensemble.
      As \textcite{malmasi_language_2015} explain, this involves aggregating the results of multiple classifiers that are all run in parallel then having a final classifier algorithm for choosing how to interpret the aggregated results and make a classification decision.
      Each classifier may use a different type of language model and/or the same type of language model on a different training set.
      A common way to generate related training sets, then, is through a process called bagging in which each set is bootstrapped from the original by sampling from the original with replacement \parencite{breiman_bagging_1996}.

      \textcite{maier_language_2014} in fact employed this method in their work on Spanish on Twitter but referred to it simply as ``voting'', which describes how the final decision is made where each classifier's decision counts as a vote.
      They use only character 5-grams and character 6-grams in their system, which leads to their best results: an overall $F_1$ of 67.72.
      \textcite{malmasi_language_2015} themselves achieved a very high accuracy of 95.54\% on the 2015 DSL competition using a classifier ensemble.
      They implemented many more models than \textcite{maier_language_2014}, however.
      For character models, they used bigrams, 4-grams, and 6-grams, and for word models, they used unigrams and bigrams.
      Similarly, \textcite{duvenhage_short_2019} achieved very good results for the 2017 DSL competition.
      His system made use of character bigrams, 4-grams, and 6-grams, as well as word unigrams and bigrams, ultimately leading to an accuracy of 98.7\%.
      Finally, \textcite{ragab_mawdoo3_2019} won \citeauthor{bouamor_madar_2019}'s (\citeyear{bouamor_madar_2019}) Arabic competition by achieving an $F_1$ of 67.2 using character $n$-grams from unigrams to 5-grams as well as word unigrams and bigrams.

      % Data augmentation
        % This is something Duvenhage (2019) suggested may improve his results further (4)
        % Marivate & Sefara (2020) suggest two types: augmentation on text source and augmentation on text representation (387)
          % Source: Provide more linguistic information on the training text, mainly semantics such as sense relations, but this can also involve round-trip translation (388-389)
        % Catch 22: Augmentation requires rules written by linguists (Marivate & Sefara 2020:387), with low-resource languages, there aren't likely to be rules, yet these are the languages you'd need to use augmentation with
          % For example, Marivate & Sefara (2020) used NLTK to add part-of-speech tags to text so that they could used WordNet to replace words with particular parts-of-speech (391)
        % Additionally, Marivate & Sefara's (2020) results for round trip translation augmentation led to very good results for news classification but comparitively poor results for hate speech classification, which relied on social media texts
          % They argued that translation may not work well on social media texts (394)
    \subsection{Classifier Algorithms}
      A language model is used to compare what one finds in a new document to what one has seen in previous documents, though this in itself does not produce a decision about how to classify a document according to its language variety.
      The decision lies with the classifier algorithm used.
      For instance, the rather unique algorithm used by \textcite{maier_language_2014} in conjunction with their byte $n$-gram model involved Lempel-Ziv-Welch (LZW) compression.
      The idea assumes that compressing the byte sequence types for different language varieties will yield different amounts of compression given different levels of entropy for each variety.
      As such, the compression in the model is compared to the compression of the new document to decide if the fit is good.
      In \citeauthor{maier_language_2014}'s (\citeyear{maier_language_2014}) case, this produced poor results, however.

      Another much more common way to decide on a classification is by using Bayes' Theorem to predict the probability of a classification given a document.
      This method assumes strong independence of features, but can yield good results nonetheless.
      \textcite{duvenhage_short_2019}, for instance, used this for part of his classifier ensemble for the 2017 DSL competition.

      While Bayes' classifiers have been used for text classification since the 1960s \parencite{mosteller_inference_1963}, a more recent development is linear support vector machines (SVMs) \parencite{cortes_support-vector_1995}.
      Linear SVMs aim to decide where a threshold between two categories should lie using soft margins.
      A natural approach might be to take observations that are known to fit into either category and then draw a threshold that is equally distant between the observations from each category that are closest to each other.
      However, this would be highly susceptible to changing due to outlier observations.
      Soft margins instead are those created by drawing a threshold that is equadistant between observations that are not at the edges of each category so as to not be impacted by outliers.
      The best soft margins to use are determined using cross-validation.

      Linear SVMs have repeatedly shown good performance for classification tasks, including for language variety identification.
      Indeed, \citeauthor{lui_classifying_2013}'s (\citeyear{lui_classifying_2013}) best results in their work on English involved not just word $n$-gram model but one that used a linear SVM to decide on the classification.
      Similarly, \textcite{rangel_low_2018} also achieved their best results by employing SVMs.
      In fact, \textcite{malmasi_language_2015} expected good enough results from SVMs that they tested no other algorithms for their submission to the 2015 DSL competition.

  \section{Conclusion}
      While it appears that perhaps the upper bounds of performance on the decision making side of the language variety identification is being reached with SVMs, there is still clearly more work to do.
      For limited domains -- and especially for traditional texts -- almost perfect accuracy has been achieved, but performance drops much lower when these systems are applied to across domains.
      Likewise, classifying very short documents still provides very real challenges for this task.
      Additionally, while it appears that researchers are interested in working on language variety identification, their goals are almost exclusively limited to identifying national varieties.
      Work on Arabic has certainly been more fine-grained by focusing on individual cities, yet from a sociolinguistic standpoint, it is known that multiple related varieties can exist in a single town where residents may be stratified by class or ethnicity or other factors.
      Effectively applying a language variety identifier to these contexts appears to be far off if not impossible. 
  \printbibliography
\end{document}
