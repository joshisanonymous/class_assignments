%%%%%%%%%%%%%%%%%%%%%%%%
% Compile with XeLaTeX %
%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
  % Packages and settings
  \usepackage{fontspec}
    \setmainfont{Charis SIL}
  \usepackage[style=apa, backend=biber]{biblatex}
    \addbibresource{References.bib}
  \usepackage{hyperref}
  % \usepackage[group-minimum-digits=4, group-separator={,}]{siunitx}

  % Document information
  \title{Review of language variety identification}
  \author{Joshua McNeill}
  \date{\today}

  % New commands
  \newcommand{\orth}[1]{$\langle$#1$\rangle$}
  \newcommand{\lexi}[1]{\textit{#1}}
  \newcommand{\gloss}[1]{`#1'}

\begin{document}
  \maketitle
  \section{Introduction}
    A task that has received significant attention in natural language processing is language identification.
    This task involves classifying documents according to their language.
    As such, language identification is also a document classification task, though there are some features that are particular to this task.
    \textcite{jauhiainen_automatic_2019} explain, whereas in other classification tasks one can tokenize into words under a single, reliable scheme, in language identification one is working with various languages which may not represent word boundaries in the same way in writing.
    Language identification is also more likely to require domain independent solutions than, for instance, classifying news articles where the sort of documents being classified is relatively consistent even if the topics are not.
    By contrast, language identification may involve documents as diverse as tweets on Twitter to blog posts to books.
    Relatedly, a single language may even be written using multiple orthographies when used in various domains.
    Where one may write \orth{u} in a tweet, the standard \orth{you} spelling is expected in formal documents, or where Roman-based pinyin may be acceptable in some contexts for Chinese writing, Chinese characters are likely required in others.
    Finally, whereas a single news article, for instance, may be given two separate classes at once, the classes for language identification are necessarily mutually exclusive \parencite[p.~678]{jauhiainen_automatic_2019}.
    Certainly, there are cases of code-switching in writing, where one can argue that two separate languages are being used simultaneously, but the goal in these cases would be either to split the document up into sub-documents by language or to simply treat the resulting mixed code as its own distinct language.

    Language identification has been worked on as far back as the 1960s with \textcite{gold_language_1967} and Mustonen \parencite[1965, as cited in][]{jauhiainen_automatic_2019}, the latter of which involved using a dictionary language model to classify individual words as English, Swedish or Finnish, which yielded an accuracy of 76\% (p.~679).
    Later, Rau (1974) introduced the idea of using the relative frequencies of unigrams and bigrams in a language to construct a language model that could be used to identify a document as being in either English or Spanish, achieving an accuracy of 89\% at the time \parencite[as cited in][p.~680]{jauhiainen_automatic_2019}.
    By the 1990s, \citeauthor{cavnar_n-gram-based_1994} (\citeyear{cavnar_n-gram-based_1994}) approach to using $n$-grams for language identification became popular as their method yielded 99.8\% accuracy when distinguishing between eight languages.
    In sum, the history of work on this task has been long and led to results that suggest that the problem has been solved, though there is more work to be done still.

    Part of what makes language identification important today is the breadth of languages used on the internet.
    For instance, although 51\% of the tweets on the social media platform Twitter are in English, the other 49\% are in a large variety of other languages.
    Indeed, Japanese makes up 19.1\% of the remainder, the second highest amount, but there is a drastic drop in the percentage that any single language makes up as one continues down the list \parencite[p.~519]{hong_language_2011}.

    Being able to automatically classify texts online into one language or another can advance understanding of how the internet is used and even shed light on social issues, but there is also practical reasons for taking on this challenge.
    In general, language identification is a first step to be taken before many other types of natural language processing can happen.
    For example, if one wishes to develop a part-of-speech tagger for Spanish, it is important to identify documents that are indeed in Spanish.
    Online texts are also a rich source for constructing corpora, particularly for low-resource languages.
    Where they may not be a literary tradition for a language, there may still be informal writing in the form of tweets, blogs, and web pages.
    Forming corpora from these resources is far easier when their presence can be detected automatically with language identification techniques.

    % Brings up the problem is classifying very short texts, as well
      % In the case of Duvenhage (2019), a short text that we'd want to classify may be 15-20 characters long (1)

    An issue that is not as often addressed in work on language identification, however, is how languages are defined to begin with.
    In general when working on this task, researchers rely on well known labels for social constructs such as ``English'' or ``Spanish'', rarely taking a critical look at this aspect of their work.
    This lack of critique is understandable given goals that are often very practical as opposed to philosophical.
    However, from a sociolinguistic point of view, just what counts as a language is a much bigger question.
    One may work under the assumption that ``English'' is a language, but this raises the question of to what extent the English of a New Yorker is the same language as the English of a South African or a Scotsman or, to take it even further, an English-based creole like Gullah along the southeast coast of the United States or Jamaican Creole.
    There is mutual intelligibility in many cases, but that does not always coincide with language delineations.
    A common example is that Swedish and Norwegian are considered two separate languages while being generally mutually intelligible, but there are many ``dialects'' of Chinese that are not mutually intelligible at all.

    For the broad task of language identification, these issues are not addressed and instead form a set of assumptions underlying researchers' work, but they are in fact taken up to a degree in work on language variety identification.
    Language variety identification narrows the task by focusing on classifying different varieties of a single language.
    ``Variety'' in this case is often taken at a national level.
    One might work on distinguishing between British, Canadian, and Australian English \parencite{lui_classifying_2013} or Argentinian, Chilean, Colombian, Mexican, and Spain Spanish \parencite{maier_language_2014,rangel_low_2018}.
    Notably, the Discriminating Similar Languages (DSL) competition involved mostly pairs of national varieties along these lines, which included distinguishing between Malay and Indonesian, Brazilian and European Portuguese, national Spanish varieties, Farsi and Dari, and Croatian, Serbian and Slovene \parencite[p.~35]{malmasi_language_2015}.
    This work does get more fine-grained, however.
    For example, \textcite{ragab_mawdoo3_2019} worked on the problem of correctly identifying 26 different dialects of Arabic, each associated with a particular city, and \textcite{malmasi_german_2017} and \parencite{ciobanu_german_2018} worked on distinguishing between four varieties of Swiss German, also each associated with a particular city.

    \subsection{Related Issues}
      The language variety identification task heightens several already difficulties that exist with language identification in general.
      One of those is the problem of low-resource langauges.
      As mentioned above, good language identification systems can be used to collect documents that can then be formed into corpora for low-resource languages that might not have formal literary traditions, but in a catch-22, it is difficult to develop a classifier for such languages without collected documents to begin with.
      Naturally, the low-resource language problem is more common when one is working with a narrower set of varieties.
      Whereas \textcite{qi_study_2019} suggsted Indonesian, Kazakh, and Tibetan as examples of low-resource languages (p.~1897), and \textcite{duvenhage_short_2019} suggested some South African languages (p.~1), one can expect effectively all varieties more fine-grained than national varieties to be low-resource in language variety identification.
      The degree to which there is a literary tradition for Punjabi-influenced West London English \parencite{sharma_style_2011} or Californian nerd girl English \parencite{bucholtz_why_1999} is unarguably limited.

      Additionally, there is perhaps more of a need to deal with code-switching when working with language variety identification than with general language identification.
      Code-switching does of course occur with language varieties at any level of abstraction -- one can speak of English-Spanish code-switching as a common phenomenon, for example \parencite{toribio_spanish-english_2002} -- though there is an argument to be made that documents containing heavy code-switching may in fact be composed of a single language variety, particularly when the switching is intra-sentential, occurring within a single sentence, whereas in more general language identification, the task here would naturally be to isolate chunks of a document as one language or another.
      Indeed, varieties such as Chiac, originating from a heavy mixture of English and French, are generally considered to be language varieties in their own right \parencite{king_chiac_2008}.
      For example, \textcite{barman_code_2014} worked on language identification in Facebook posts by multilinguals where Bengali, Hindi, and English were present (p.~14).
      As they were starting from a broad conceptualization of what a language is, they described their task as separating out the languages of any given post even when the code-switching occurs from word to word (p.~13), a situation that is difficult to distinguish from borrowings of individual words into a language.\footnote{
        See \textcite{deuchar_english-origin_2016} for a relatively recent overview of the arguments in sociolinguistics.
      }

      In particular, when working with language identification, it is common to apply these systems to internet domains where code-switching is also common.
      English-Chinese bilinguals have been documented regularly code-switching as far back as when ICQ, a relatively archaic instant messenger by today's standards, was still widely used \parencite{ho_functional_2006}.
      The practice also occurs even in independent chat rooms deployed for experiments, as \textcite{cardenas-claros_code-switching_2009} found in their work with English-Spanish and English-In\-do\-ne\-sian bilinguals.
      This code-switching can take the form of inter-sentential switching as Hidayat (2012) found on Facebook \parencite[as cited in][p.~14]{barman_code_2014}, where adjacent sentences are in different languages, but intra-sentential switching can also be prevalent, as \textcite{san_chinese-english_2009} found in blogs written by Chinese-English bilinguals in Macao, China (p.~12).

      The presence of code-switching at times can exacerbate language variety identification even further.
      As already mentioned, having to work with multiple orthographies and writing norms is a particular issue for this task over other classification tasks.
      \textcite{barman_code_2014} found a special case of this where their participants would sometimes write Bengali and Hindi using the Roman alphabet, where they would normally be written in other scripts, to facilitate typing where English was also present (p.~13).
      Their solution to this was to exclude anything not written this way so as to avoid dealing with multiple scripts \parencite[p.~15]{barman_code_2014}.
      This solution is not ideal but perhaps helps when dealing with multiple scripts.
      More such solutions and approaches to language variety identification will be covered in the next section.

      % Orthographies
        % This is not uncommon for various scripts and even different encodings, as Jauhiainen et al. (2019) acknowledge (682-683)
      % Where multiple spellings are possible, these can be normalized
        % Barman et al. (2014) used the Lexical Normalization List (LexNormList) (Han et al. 2012) to do this (17)

  \section{Approaches}
  % Approaches to similar language identification problems
    \subsection{Corpora}
    % Annotations and corpora used
      % Twitter
        % Used for one task in Bouamor et al.'s (2019) Arabic competition
          % Constructed by search for users who used hashtags for the desired countries (in English apparently) (201)
      % MADAR Travel Domain Corpus is a parallel corpus for Arabic from different cities (Bouamor et al. 2019:201)
      % The Discriminating between Similar Languages (DSL) dataset was part of a competition Zampieri et al. (2017)
        % 14 languages, 22,000 news texts for each (Zampieri et al. 2017:3)
      % WordNet is an English lexicon linking words through synonymy, useful for text augmentation approaches (Miller 1995)
      % Lui & Cook (2013) didn't have a good corpus option for Australian English and so built one from the web focusing on .au domain names (9)
        % Specifically, they took frequent words from the BNC and used BootCaT to generate 3-tuples that were then used to send search engine queries. They saved and processing the resulting pages (9)
        % They also did this to expand their British and Canadian corpora and to obtain government pages from each country (9)
      % Maier & Gómez-Rodriguez (2014) constructed their own corpus from Twitter for Spanish varieties (25-27)
        % Franco-Salvador et al. (2015) used the HispaBlogs dataset instead
        % By Rangel et al.'s (2018) work, they also had access to HispaBlog for a similar dataset (160)
      % Barman et al (2014) annotate sentences as monolingual or mixed, then for mixed, annotate where the language changed (15)

    \subsection{Language Models}
    % Dictionary approach
      % Barman et al. (2014) achieved 93.12% accuracy using training data and LexNormList as dictionaries (19)
    % Character n-gram approach
      % Barman et al. (2014) achieved 94.62% accuracy with character n-grams alone, and this when very slightly up when included other context-free features as well (19)
        % These also went up roughly another 1% when adding context by using Conditional Random Fields (Barman et al. 2014:20)
      % Maier & Gómez-Rodriguez (2014), using the TextCat implementation of Canar & Trenkle (1994), saw very poor results for the five varieties of Spanish they looked at on Twitter with F1 of 43.93 for Argentina being the highest (20% is chance) and the overall F1 being 34.08 (28)
        % Maier & Gómez-Rodriguez (2014) believe this is due to short tweets resulting in a lack of n-grams to match (28)
          % This is supported when they look at F1 scores as a function of tweet length and find the longer the tweet, the better the F1 (Maier & Gómez-Rodriguez 2014:29)
        % ALso, with absolute discounting starting at 6-grams, overall F1 of 66.96 (Maier & Gómez-Rodriguez 2014:29)
    % Syllable n-gram approach
      % Maier & Gómez-Rodriguez (2014) used the TIP syllabifier (Spanish Academy-based) to perform syllabification (30)
        % F1 reached 57.88 overall with 4-grams and discounting (30)
    % Word-based approaches
      % Using linear SVM, Lui & Cook (2013) found this to outperform other methods (i.e., bag-of- function -words, part-of-speech + function words, byte sequences, spelling variant pairs) (11)
        % They tried different types of corpora, and Twitter data was by far the hardest to classify (11)
      % Word Skip-grams were used by Franco-Salvador et al. (2015) for Spanish language variety identification and resulted in 0.722 accuracy (10)
      % Rangel et al. (2018) focus on words but develop low dimensionality representation (LDR) to filter out most words and arrive at the 6 key words thought to be representative of each variety
        % The process: Perform a series of weight calculations, starting with TF-IDF (158-159)
        % with only 30 features, they outperformed weighted word bigrams and character 4-grams with .711 accuracy (164)
          % Just below results for skip-gram approach with 300 features, though the difference wasn't statistically significant
    % Compression approach
      % Bush (2014) used LZW compression (Welch 1984) to achieve between 60% and 100% accuracy on a standard langauge identification task (2)
        % General idea is that where there is less entropy, it's easier to compress data, but Bush's (2014) work was only exploratory
      % Maier & Gómez-Rodriguez's (2014) implementation for Spanish tweets of different varieties resulted in results that only bested a plain frequency character n-gram model but otherwise performed worse than character n-gram models with discounting and syllable models (31)
        % This also had the problem of very short tweets achieving the same exact compression rates in many instances (Maier & Gómez-Rodriguez 2014:31)
    % Ensembles of classifiers
      % Put to use by Maier & Gómez-Rodriguez (2014) but referred to as "voting", achieving 67.72 F1 overall for their Spanish varieties on Twitter task (31)
        % This is higher than what they achieved through individual classifiers alone.
      % Multiple classifiers can be run in parallel on some input with their individual results then aggregated (Malmasi & Dras 2015:36-37)
        % A common way to generate multiple classifiers is through bagging, which is bootstrapping new training sets by sampling from one training set with replacement (Breiman 1996), after which different classifiers can be trained from each set (Malmasi & Dras 2015:37)
        % The results of classifiers can be combined in various ways to lead to a decision
      % Malmasi & Dras (2015) achieved 95.54% accuracy on DSL '15 with this using character 2-, 4-, and 6-grams and word 1- and 2-grams as classifiers (40)
      % Duvenhage (2019) achieved very high accuracy -- 98.7% on DSL '17 -- by using a naive Bayes classifier to identify the language family and then a lexicon-based classifier to identify languages int aht family
      % Ragab et al. (2019) used an ensemble for fine-grained city-level Arabic variety identification, achieving 67.2 F1 on the MADAR task from Bouamor et al. (2018) (247).
        % Their features were word uni- and bi-grams and character n-grams from 1 to 5, all TF-IDF weighted (245)
    % Data augmentation
      % This is something Duvenhage (2019) suggested may improve his results further (4)
      % Marivate & Sefara (2020) suggest two types: augmentation on text source and augmentation on text representation (387)
        % Source: Provide more linguistic information on the training text, mainly semantics such as sense relations, but this can also involve round-trip translation (388-389)
      % Catch 22: Augmentation requires rules written by linguists (Marivate & Sefara 2020:387), with low-resource languages, there aren't likely to be rules, yet these are the languages you'd need to use augmentation with
        % For example, Marivate & Sefara (2020) used NLTK to add part-of-speech tags to text so that they could used WordNet to replace words with particular parts-of-speech (391)
      % Additionally, Marivate & Sefara's (2020) results for round trip translation augmentation led to very good results for news classification but comparitively poor results for hate speech classification, which relied on social media texts
        % They argued that translation may not work well on social media texts (394)
    \subsection{Classifier Algorithms}
  \printbibliography
\end{document}
