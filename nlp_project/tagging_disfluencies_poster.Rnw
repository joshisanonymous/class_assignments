\documentclass{beamer}
    % Packages and settings
    \usetheme{RedLion}
    \usepackage[orientation=landscape, size=a0, scale=1.8]{beamerposter}
    \usepackage{fontspec}
        \setmainfont{Charis SIL}
    \usepackage[style=apa, backend=biber]{biblatex}
        \addbibresource{References.bib}
    % \usepackage{listings}
    %     \lstset{basicstyle=\ttfamily,
    %             breaklines=true}
    % \usepackage{graphicx}
    % \usepackage{hyperref}
    %     \hypersetup{colorlinks=true,
    %                 allcolors=blue}

    % Additional commands
    \newcommand{\orth}[1]{$\langle$#1$\rangle$}
    \newcommand{\lexi}[1]{\textit{#1}}

    % Document information
    \title{Tagging disfluencies in sociolinguistic interview data}
    \author{Joshua McNeill}
    \institute{University of Georgia}
    \date{4 December 2019}

\begin{document}
  <<settings_load_scripts, echo = FALSE>>=
  read_chunk("tagging_disfluencies.R")
  opts_chunk$set(echo = FALSE,
                 warning = FALSE,
                 message = FALSE,
                 results = "asis",
                 fig.height = 20,
                 fig.width = 8)
  @
  <<preamble>>=
  @
  \begin{frame}[t]{}
    \begin{columns}
      \column{0.29\linewidth}
        \begin{block}{Introduction}
          Part of speech tagging refers to the process of assigning lexical categories to lexical expressions in a text, either manually or automatically.
          In natural language processing, the aim is to perform this task automatically, often through the use of \emph{n}-gram language models that provide the probability distribution for words receiving various part of speech tags given some previous context.
        \end{block}
        \begin{block}{Problem}
          One major area in which spoken language differs from written language is in the presence of disfluences such as repetitions, hesitations, and repairs.
          Repairs are made up of four parts \parencite{levelt_monitoring_1983}:
          \begin{itemize}
            \item The reparandum
            \item A moment of interruption
            \item An editing term
            \item The repair
          \end{itemize}
          While these disfluencies can by normalized out of the data before tagging, in many cases, the disfluencies themselves may be the items of interest, and so removing them is undesirable.
        \end{block}
        \begin{block}{Methodology}
          Four 30 minute sociolinguistic interviews were conducted with speakers of Louisiana French in 2019.
          These interviews were transcribed and manually tagged.
          Tags within reparanda chunks then had \texttt{\_DIS} appended to them.
          NLTK was used to create a trigram tagger from 90\% of the sentences ($N$ = \input{./stats/train_size.tex}) with bigram, unigram, and default taggers as backoff taggers.
        \end{block}
      \column{0.44\linewidth}
        \begin{block}{Example Text}
          \texttt{tout/JJ ça/PRP est/VBZ pour/IN \textbf{uh/UH\_DIS les/DET\_DIS les/DET\_DIS les/DET\_DIS uh/UH\_DIS} les/DET maringouins/NNS pouvaient/VBD pas/RB te/PRP manger/VB ./. yup/RB yup/RB uh/UH\_DIS vous\_autres/PRP\_DIS vous\_autres/PRP mange/VBZ les/DET écrevisses/NNS}
        \end{block}
        \begin{block}{Evaluation}
          \parbox{0.45\linewidth}{
            <<compare_dis_nodis>>=
            @
          }
          \parbox{0.45\linewidth}{
            \begin{tabular}{l r r r r}
              Tag          & Precision & Recall & F\textsubscript{1} score & N \\
              \hline
              CC           & 0.80      & 0.89   & 0.84 &  27 \\
              DT           & 0.90      & 0.38   & 0.53 &  48 \\
              IN           & 0.78      & 0.89   & 0.83 &  53 \\
              JJ           & 0.79      & 0.65   & 0.71 &  23 \\
              NN           & 0.40      & 0.92   & 0.56 &  37 \\
              NNP          & 1.00      & 0.81   & 0.90 &  16 \\
              NNS          & 1.00      & 0.60   & 0.75 &  15 \\
              PRP          & 0.83      & 0.78   & 0.81 &  69 \\
              RB           & 0.91      & 0.89   & 0.90 &  88 \\
              UH           & 0.92      & 1.00   & 0.96 &  22 \\
              VB           & 0.86      & 0.86   & 0.86 &   7 \\
              VBD          & 1.00      & 0.70   & 0.82 &  10 \\
              VBN          & 1.00      & 0.50   & 0.67 &  10 \\
              VBP          & 0.76      & 0.76   & 0.76 &  21 \\
              VBZ          & 0.85      & 0.68   & 0.76 &  25 \\
              WP           & 0.41      & 0.64   & 0.50 &  11 \\
              DT\_DIS      & 0.75      & 0.20   & 0.32 &  15 \\
              PRP\_DIS     & 0.28      & 0.31   & 0.29 &  16 \\
              UH\_DIS      & 1.00      & 0.98   & 0.99 &  65 \\
              VBD\_DIS     & 0.33      & 0.50   & 0.40 &   2 \\
              VBZ\_DIS     & 0.38      & 0.50   & 0.43 &   6 \\
              WP\_DIS      & 1.00      & 0.12   & 0.22 &   8 \\
              \hline
              accuracy     &           &        & 0.72 & 683 \\
              macro avg    & 0.50      & 0.43   & 0.44 & 683 \\
              weighted avg & 0.78      & 0.72   & 0.73 & 683 \\
            \end{tabular}
          }
        \end{block}
      \column{0.27\linewidth}
        \begin{block}{Discussion}
          This modification of the training data is not expected to improve the accuracy of taggers beyond what is already possible.
          Indeed, a few attempts have already been made to improve accuracy by incorporating prosodic features from the original audio \parencite[e.g.,][]{christodoulides_dismo:_2018}.
          Unsurprisingly given the size of the training data used here, the accuracy of the tagger actually goes down when reparanda chunks are tagged as disfluencies.
          It is not clear if this decrease in accuracy is purely a function of the data being made more sparse by the addition of twice as many parts-of-speech.
          It is perhaps possible that a much larger training set would reverse this result.

          On linguistic grounds, there is also a question of whether disfluencies can be accurately identified purely through short sequences of words without recourse to prosody as has been used elsewhere.
          Again, this seems to not be possible, at least not with such a small set of training data.
          The F\textsubscript{1} scores for all of the \texttt{DIS} tags are fairly low except for those tagged as interjections (i.e., \texttt{UH}), as these were almost universally seen as disfluencies to begin with.
        \end{block}
        \begin{block}{Conclusion}
          Two conclusions can be drawn from this attempt at dealing with disfluencies:
          \begin{itemize}
            \item Without a much larger training set, tagging accuracy decreases
            \item Identifying disfluencies purely through short sequences of words may not be possible
          \end{itemize}
        \end{block}
    \end{columns}
  \end{frame}
\end{document}
